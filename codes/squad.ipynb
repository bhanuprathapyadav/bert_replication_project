# Install required dependencies
!pip install transformers datasets torch --quiet

# Import required libraries
from transformers import BertTokenizer, BertForQuestionAnswering
from datasets import load_dataset
import torch
import random

# Load the fine-tuned BERT model and tokenizer from Hugging Face
model_name = "deepset/bert-large-uncased-whole-word-masking-squad2"
model = BertForQuestionAnswering.from_pretrained(model_name)
tokenizer = BertTokenizer.from_pretrained(model_name)

# Load the SQuAD dataset
squad_dataset = load_dataset('squad')

# Randomly select an example from the dataset
random_example = random.choice(squad_dataset['train'])

# Extract the question and context from the selected example
question = random_example['question']
context = random_example['context']

# Tokenize the input (question + context)
input_ids = tokenizer(question, context, return_tensors='pt').input_ids

# Perform inference (use GPU if available)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)
input_ids = input_ids.to(device)

# Get model predictions
with torch.no_grad():
    outputs = model(input_ids)
    start_scores = outputs.start_logits
    end_scores = outputs.end_logits

# Get the most likely start and end positions of the answer
answer_start = torch.argmax(start_scores)
answer_end = torch.argmax(end_scores) + 1

# Decode the answer
answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))

# Output the question and the predicted answer
print(f"Question: {question}")
print(f"Answer: {answer}")

