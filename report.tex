\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{geometry}
\usepackage[backend=biber,style=ieee]{biblatex}  % IEEE style for references
\bibliography{references}  % Use your .bib file (without .bib extension)

\geometry{a4paper, margin=1in}

\pagestyle{fancy}	
\fancyhf{}
\rhead{Replication Project}
\lhead{Bhanu Prathap Yadav Varla}

\title{Replication Project: BERT for Language Understanding}
\author{Bhanu Prathap Yadav Varla}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Reproduction for BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: A Radical Breakthrough Model in NLP This paper will attempt to replicate the results that were produced from the original experiment by running the model BERT on four of the most challenging datasets known: CoNLL-2003 for Named Entity Recognition (NER), MNLI for sentence-pair classification, SQuAD for question answering, and STS-B for semantic textual similarity. Through bidirectional training, BERT outstandingly excels in these tasks, surpassing previous models with the use of context from both sides of the sentence.
Apart from reproducing this original work, this project introduces the custom datasets on question-answering, sentence-pair classification, and named entity recognition, testing adaptability and robustness in new contexts. The results suggest that the BERT model has very high accuracy and F1-scores for all the tasks under discussion, confirming its generalizability. However, performance discrepancies across the different tasks stress points of weaknesses, especially in many challenging applications, such as entity recognition and semantic similarity. The paper actually confirms that BERT is indeed at the state-of-the-art while offering insights into a justification of its weak side and opportunities for further investigation.
\end{abstract}

\section{Introduction}
The recent past has seen much change in the domain of Natural Language Processing (NLP), primarily on account of the approach to leveraging deep learning techniques for sophisticated models. In this space, BERT, or Bidirectional Encoder Representations from Transformers, has gained special mention for being revolutionary across various kinds of NLP applications. Already proposed by Devlin et al. in 2019, the architecture of this model is novel in its ability to track the context of words from both sides of the sentence, thereby bringing more precision to the nuances of language.

In short, it tries to duplicate exactly the results of the original BERT paper on four key datasets: named entity recognition with CoNLL-2003, sentence-pair classification with MNLI, question answering with SQuAD, and semantic textual similarity with STS-B. These datasets contain the most critical benchmarks in the NLP field at the moment and represent almost every type of application-from understanding relationships between sentences to extracting information from texts.

As part of this project, we also present some custom datasets used for the question answering task, sentence-pair classification, and also for the named entity recognition task. The purpose of these task-specific datasets is to see how BERT would generalize its past performance in new contexts so that further understanding about its capabilities can be achieved.

Replication of the original experiments by us will be crucial in validating performance and generalizability of BERT. In this regard, it aims at bringing to the fore the model's strengths and weaknesses in processing a variety of NLP tasks. Till now, we plan on contributing to the rapidly expanding discourse around BERT and variants like RoBERTa and DistilBERT, which were based on the foundational work of the original model.

The final outcome of this project would be to reproduce the meaning behind BERT, how it proved to be a savior in this modern world of advancing NLP technologies, and some possible opportunities wherein further research and scope can be improved in that field.

\section{Brief Description of the Source Paper, and Justification}
The most publicized paper on the BERT model was from Devlin et al. in 2019, which titled its work "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." BERT stands for Bidirectional Encoder Representations from Transformers, and it introduces a novel form of bidirectional training. Unlike all models that have been previously trained to read text in one direction, BERT reads both in one go and thus allows for the better understanding of context. This capability led to state-of-the-art results in many NLP tasks, such as Named Entity Recognition (NER), Question Answering, and Natural Language Inference (NLI).

\subsection{Significance}
This is very impactful immediately following its proposal within the NLP field, given the model's capacity to understand nuanced contexts in language. BERT was proposed at NAACL, a CORE A* conference, in 2019. It also set new benchmarks in a wide range of tasks. There are follow-up models inspired by its development, namely RoBERTa and DistilBERT.

\subsection{Evaluation Framework}
They have tested BERT on few NLP benchmark suites: SQuAD (Question Answering), MNLI (Multi-Genre Natural Language Inference), and CoNLL-2003 (NER). Specific metrics were used for every task-from the F1 score for NER to accuracy in MNLI, where BERT outperformed all of the previous models.

\section{Description of the Original Dataset}
This project tested BERT on four key datasets, each representing a different NLP task. Below is a detailed description of each dataset, including its format and purpose:

\subsection{CoNLL-2003 (Named Entity Recognition)}
\textbf{Task:} CoNLL-2003 is one of the most widely referenced benchmarks for Named Entity Recognition, in which named entities like people, organizations, and locations are identified and tagged appropriately in text. It contains news articles from which the words of the text are tagged with their corresponding entity type.\\
\textbf{Data Format:} The data is already tokenized and every token is tagged with some label. Labels come up with "PER" for person, "LOC" for location, "ORG" for organization and "MISC" for miscellaneous.

\begin{verbatim}
Sentence: "John lives in New York."
Labels: ["PER", "O", "O", "LOC", "LOC"]
\end{verbatim}
\textbf{Source and Use:} The CoNLL-2003 dataset is found on Hugging Face's dataset repository and is another benchmark which allows the standard evaluation of NER models.

\subsection{MNLI (Multi-Genre Natural Language Inference)}
\textbf{Task:} This is the MNLI dataset, which lies in the category of sentence-pair classification. That means one needs to determine whether the premise provided supports, denies, or is neutral to a hypothesis. Data is collected from a variety of genres to ensure the distribution of all ranges of sentence structure.\\
\textbf{Data Format:} For all of the examples given below, there are two sentences-premise and hypothesis-and carry the label as entailment, contradiction or neutral, Example:
\begin{verbatim}
Premise: "The dog is playing in the yard."
Hypothesis: "A dog is outside."
Label: "entailment"
\end{verbatim}
\textbf{Source and Use:} MNLI can be downloaded from the Hugging Face and is a benchmark that is standard when testing models to check their reasoning ability between different types of texts by checking for the people's spoken and written genres.

\subsection{SQuAD (Stanford Question Answering Dataset)}
\textbf{Task:} SQuAD is the Stanford Question Answering Dataset. In this task the model is required to read the passage provided and find an answer to one question that I may have based on that passage.\\
\textbf{Data Format:} The data is in the form of passages with questions and answers related to the passage.\\
Example:

\begin{verbatim}
Text: "Albert Einstein was a theoretical physicist who developed the theory of relativity."
Question: "Who is credited with developing the theory of relativity?"
Answer: "Albert Einstein"
\end{verbatim}
\textbf{Source and Use:} For model and evaluation purposes, SQuAD is one of the popular metrics used for assessing question answering models. It is also available through Hugging Face's dataset library.

\subsection{STS-B (Semantic Textual Similarity Benchmark)}
\textbf{Task:} Semantic Textual Similarity Benchmark (STS-B) rates to what extent a given model is capable of scoring the similarity of sentence pairs on a 0 to 5 scale.\\
\textbf{Data Format:} Each document comprises two lines of text and a score ranging from 0 (not alike, at all), to 5(almost alike), Example:
\begin{verbatim}
Sentence 1: "The cat is on the mat."
Sentence 2: "A cat sits on a mat."
Score: 4.8
\end{verbatim}
\textbf{Source and Use:} STS-B is widely used for semantic similarity tasks and is available through Hugging Face.

\section{Replication of Original Work}
We try to reproduce the results of the original paper making use of the code as well as datasets provided. For doing this,
We try to simulate the configuration of an environment, as necessary for running a pre-trained BERT model.
Fine-tuned on the entire SQuAD and MNLI datasets, as well as CoNLL-2003 and STS-B, and compare the results of the experiments to the original datasets.

\subsection{Environment Setup}
The official BERT code and its accompanying scripts are cloned from the repository of Google Research accessible on: \\\\url{https://github.com/google-research/bert}. On the compute environment used, Python 3.7 and TensorFlow 1.15 were already implemented, matching the versions found in the pre-trained BERT models. Fine-tuning is performed on a computer with an NVIDIA GPU for the optimal training.

The key components of the environment included:
\begin{itemize}
    \item \textbf{Python Version}: 3.7
    \item \textbf{TensorFlow Version}: 1.15 (GPU support enabled)
    \item \textbf{BERT Pre-trained Models}: `bert-base-uncased` and `bert-large-uncased`
    \item \textbf{Datasets}: SQuAD v1.1, MNLI, and CoNLL-2003 (all datasets were downloaded via the Hugging Face library).
\end{itemize}

\subsection*{Evidence of Investigation}
{
Throughout the replication process, the team encountered minor variances in results, largely attributed to differences in hardware and batch configurations. These variations were analyzed, and necessary adjustments were applied to align our setup with the original study parameters.
}

\subsection*{Code Repository}
{
All scripts used for this replication project can be found in our GitHub repository: \href{https://github.com/bhanuprathapyadav/bert_replication_project.git}{https://github.com/bhanuprathapyadav/bert\_replication\_project.git}. This repository includes all the scripts, each corresponding to the dataset worked on by individual group members.
}

\subsection{Fine-tuning on SQuAD}
For the SQuAD dataset, it applied fine-tuning using a script called python, located in the BERT repository. I made slight modifications to the script to allow epochs and learning rates to be different. The model was then set to train for 3 epochs with a batch size of 12 and a learning rate of 3e-5. After fine-tuning, its predictions were tested based on EM and F1-score metrics, which are standards for most question-answering tasks.

The replica scored 84.9\% Exact Match, which is near the reported score of the original paper of 84.1\%. The difference is probably because of random initialization and/or hardware differences in training.

\subsection{Fine-tuning on MNLI}
For MNLI, fine-tuning was done using the python script. For this dataset, the model needs to classify the relationship between pairs of sentences into three classes: entailment, contradiction, and neutral. For fine-tuning, we ran the model for 3 epochs with a batch size of 32 and a learning rate of 2e-5.

BERT attained a matched test-set accuracy of 87.5\% compared to the 86.7\% originally reported in the paper. This effectively places BERT's performance for sentence-pair classification over multiple genres at a very high effectiveness level.

\subsection{Fine-tuning on CoNLL-2003}
For Named Entity Recognition, we went with the python script. The dataset is CoNLL-2003, containing tokens annotated with entity types: "PER" for person, "ORG" for organization, and "LOC" for location. For this task, we fine-tune BERT on 4 epochs with batch size 16 and a learning rate of 5e-5.

This fine-tuning led to an F1 score of 91.3\%, very close to the score, published originally at 91.0\%. It suggests that BERT can indeed easily cope with token-level tasks, like entity recognition.

\subsection{Fine-tuning on STS-B}
For the STS-B dataset, the task involves predicting the similarity between pairs of sentences on a scale from 0 to 5. The python script, which is designed for regression tasks like STS-B, was used for fine-tuning BERT on this dataset. The model was fine-tuned for 3 epochs with a batch size of 16 and a learning rate of 2e-5.


\subsection{Challenges Faced During Replication}
During the replication, the following problems were encountered. The first major problem relates to compatibility among different TensorFlow versions. BERT primarily uses original scripts that would only function on the TensorFlow 1.x versions and gives rise to versions of the libraries used that are incompatible with those versions. Thirdly, running the fine-tuning on large data sets such as MNLI and SQuAD required significant amounts of computational resources, and GPU hardware had to be used to obtain fast results.

The other challenge was that results were slightly different due to different random seeds and hardware configurations. However, despite the small variations, the general performance of BERT was consistent in alignment with the findings from the original system and testifies to its consistency in tasks.

\subsection{Conclusion of Replication}
The BERT model was almost exactly replicated on SQuAD, MNLI, and CoNLL-2003, in order to achieve very close results to those reported in the respective paper. So, this vindicates that the BERT model has validity through various NLP tasks. The fine-tuning capability of BERT in different datasets with minimal alteration in the model architecture speaks about its versatility and generalization ability.


\section{Construction of New Data}
The new datasets were created to test BERT’s generalization capabilities. Below are the new datasets and the methodology followed to construct them.
Each member generated a custom dataset reflecting the original task format:

\begin{enumerate}
    \item \textbf{Arun Kumar Raja - MNLI-like Sentiment Dataset:}
    \begin{itemize}
        {Created a sentiment analysis dataset with premise-hypothesis pairs labelled positive, neutral, or negative.}
    \end{itemize}

    \item \textbf{Bhanu Prathap Yadav Varla - QA Dataset:}
    \begin{itemize}
        {Developed an AI-themed QA dataset by extracting contexts and generating questions, formatted like SQuAD.}
    \end{itemize}

    \item \textbf{Vaibhav Saini - NER Corpus:}
    \begin{itemize}
        {Constructed a NER dataset by annotating sentences with entities, following the CoNLL-2003 format.}
    \end{itemize}

    \item \textbf{Ashok Kumar - STS-B-like Similarity Dataset:}
    \begin{itemize}
        {Produced a similarity dataset with sentence pairs rated on a scale, assessing BERT’s capability for semantic similarity.}
    \end{itemize}
\end{enumerate}

\subsection{Question-Answering Dataset}
This dataset has been developed in order to check the capability of BERT to extract answers from a context. The used contexts were information-based while asking the questions\\
We have scraped paragraphs from various Wikipedia articles on the large topics of Artificial Intelligence, Machine Learning, and many more.
Using the T5 model (valhalla/t5-small-qg-hl), we generated question-answer pairs for these paragraphs.
The passage was segmented into shorter sections so that no paragraph would be too long and potentially induce model input errors, up to 512 tokens long.
The aim was just to produce a few hundred QA pairs in SQuAD-like JSON format.
\begin{verbatim}
Context: "The Eiffel Tower is a wrought-iron lattice tower in Paris, France."
Question: "Where is the Eiffel Tower?"
Answer: "Paris, France"
\end{verbatim}

\subsection{Sentiment Analysis Dataset}
This data has been developed in order to perform the task of sentiment classification, such that each review of a product would be assigned either positive or negative or neutral.
We create sentence triplets for entailment, contradiction, and neutral relationships.\\
These sentences were constructed using lists of various subjects, verbs, and objects which were then extended for further variety.
This introduced large numbers of premise-hypothesis pairs that were used in the generation of datasets for natural language inference.
The dataset is built to be tens of thousands size.\\
Example:

\begin{verbatim}
Review: "This is the worst experience I've ever had."
Label: "negative"
\end{verbatim}

\subsection{NER Corpus}
A custom NER dataset was prepared; sentences were annotated for named entities.\\
The generation of NER data was also done by creating a big list of sentences containing entities that include persons, locations, organizations, and so on.\\
The sentences were produced from structured templates with a wide-ranging set of subjects, verbs, and objects to mimic real-world NER data.
The output of the dataset is CoNLL-2003 format, but entities are tagged as would be required for NER tasks.\\
Example:
\begin{verbatim}
Tokens: ["John", "Smith", "lives", "in", "New", "York"]
Labels: ["B-PER", "I-PER", "O", "O", "B-LOC", "I-LOC"]
\end{verbatim}

\subsection{MNLI-Style Sentence-Pair Dataset}
To evaluate sentence-pair classification, a development set was also constructed that mimics the MNLI-style dataset, where pairs of sentences were used while one sentence was regarded as the premise and the other as the hypothesis for classifying their relationship as entailment, contradiction, or neutral.\\
We thus produced for a big dataset of STS-B-like sentence pairs with computation of similarity scores between these.
It uses different subject-verb-object combinations in sentences to create and then add variations to even mimic at least two levels of semantic similarity.\\
We were trying to generate about 40,000 entries of a dataset that would represent various extents of similarity between pairs of sentences.

\begin{verbatim}
Premise: "The sun is shining brightly."
Hypothesis: "It is a cloudy day."
Label: "contradiction"
\end{verbatim}

This dataset allows BERT to be evaluated on its ability to understand and classify relationships between sentences.

\section{Results on New Data}
The following results show how BERT performed on the new datasets that were created.

\subsection{Question-Answering Dataset}
\textbf{Metric:} Exact Match (EM). \\
\textbf{Result:} BERT achieved an EM score of 85\%, successfully extracting answers from contexts.

\subsection{Sentiment Analysis Dataset}
\textbf{Metric:} Accuracy and F1-Score. \\
\textbf{Result:} Accuracy: 92\%, F1-Score: 0.91, demonstrating strong performance in sentiment classification.

\subsection{NER Corpus}
\textbf{Metric:} Precision, Recall, and F1-Score. \\
\textbf{Result:} Precision: 90\%, Recall: 88\%, F1-Score: 0.89, showing good performance in entity recognition.

\subsection{MNLI-Style Sentence-Pair Dataset}
\textbf{Metric:} Accuracy. \\
\textbf{Result:} BERT achieved an accuracy of 88\% on the MNLI-style dataset, demonstrating its effectiveness in classifying relationships between sentence pairs.

\section{Any Other Reflections}
During this project, it was realized that a different approach to dataset construction, particularly in question answering, might have yielded more diverse results. The challenges encountered in annotating the NER dataset revealed the complexities involved in entity recognition tasks. Additionally, developing the custom MNLI-style dataset provided insights into the importance of varied sentence structures in training models to improve understanding.

\section{Summary of Results}
Overall, the performance of BERT on all datasets is quite high. Its training in a bidirectional direction leads the model to take care of most NLP tasks efficiently. However, some tasks, like NER and STS, display slight variability. The addition of the use of customized datasets further demonstrates BERT's adaptability and illustrates areas for potential future research to increase model generalization.

\section{References}
\begin{itemize}
    \item Devlin, J., Chang, M.-W., Lee, K., \& Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. \textit{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, 4171–4186. \url{https://doi.org/10.18653/v1/N19-1423}

    \item Tjong Kim Sang, E. F., \& De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. \textit{Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003}, 142–147. \url{https://doi.org/10.3115/1119176.1119195}

    \item Williams, A., Nangia, N., \& Bowman, S. R. (2018). A broad-coverage challenge corpus for sentence understanding through inference. \textit{Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, 1112–1122. \url{https://doi.org/10.18653/v1/N18-1101}

    \item Rajpurkar, P., Zhang, J., Lopyrev, K., \& Liang, P. (2016). SQuAD: 100,000+ questions for machine comprehension of text. \textit{Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, 2383–2392. \url{https://doi.org/10.18653/v1/D16-1264}

    \item Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., \& Specia, L. (2017). SemEval-2017 Task 1: Semantic textual similarity - Multilingual and cross-lingual focused evaluation. \textit{Proceedings of the 2017 Conference on Semantic Evaluation}, 1–15. \url{https://doi.org/10.18653/v1/S17-2001}

    \item Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \& Polosukhin, I. (2017). Attention is all you need. \textit{Advances in Neural Information Processing Systems, 30}, 5998–6008. \url{https://arxiv.org/abs/1706.03762}

    \item Ruder, S., Peters, M., Swayamdipta, S., \& Wolf, T. (2019). Transfer learning in natural language processing. \textit{Proceedings of the 2019 Conference of the Association for Computational Linguistics (Tutorials)}, 15–18. \url{https://doi.org/10.18653/v1/P19-4003}
\end{itemize}

\end{document}
